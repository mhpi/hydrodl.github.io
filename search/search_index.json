{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HydroDL","text":"<p>HydroDL is a coding library designed to model hydrologic systems using two classes of models: (1) purely data-driven neural networks and (2) hybrid, physics-informed differentiable (\\(\\delta\\)) models. Applications range from soil moisture to streamflow to water quality and ecosystem. Through this portal, we collect all of our published models so you can navigate through them easily. These codes are developed by the Multiscale Hydrology, Processes and Intelligence (MHPI) group headed by Dr. Chaopeng Shen, faculty member at Penn State Univ (http://water.engr.psu.edu/shen). Codes are contributed by the whole MHPI team (https://sites.google.com/view/mhpi/team).</p> <p>The two model classes each have their own advantages. Purely data-driven models are great because they are: (i) easy to train and run at scale. (ii) automatically adaptable to predictable errors in forcings (iii) highly efficient in computation. (iv) not heavily prone to human bias in model selection.  </p> <p>Differentiable models mix process-based equations (called priors) and neural networks (NNs) at the fundamental level so they can be trained together in one stage (called \"end-to-end\"). This way, the network components can be supervised indirectly by outputs of the combined system, and do not necessarily need supervising data for its direct outputs. Differentiability can be supported by automatic differentiation, adjoint (Song et al., 2023, https://hess.copernicus.org/preprints/hess-2023-258/), or any other method that can produce the gradients of loss with respect to large amounts of parameters efficiently. Such models can train one neural network using big data, improving the generalizability, robustness, and complexity of the learned relationships.   </p> <p>While being able to match purely data-driven models in performance in even data-dense regions, differentiable models have the following advantages: (1) Interpretable: they represent the full physical processes and output intermediate physical variables not used in training, e.g., ET, recharge, snow water equivalent, which help to provide an explainable narrative. (2) Generalizability: because of the use of priors, differentiable models often generalize better under data-sparse regions like in global hydrologic simulations. (Feng et al., 2023a, https://doi.org/10.5194/hess-27-2357-2023; 2023b, https://gmd.copernicus.org/preprints/gmd-2023-190/) (3) (Under investigation) Extremes: forced by physical priors, the differentiable models may be better at representing unseen extreme scenarios and future trends. (4) Knowledge Discovery: we can use NNs as question marks in the combined system to learn unknown/uncertain relationships from data. In this sense, the priors serve to constrain or narrow down the scope of learning so we can understand the learned relationships. (5) Respect physical laws and sensitivities: we can force the model to respect laws such as mass balance, energy-driven evapotranspiration, etc. so our model has the correct sensitivities.</p>"},{"location":"Contribute/","title":"Contribute","text":""},{"location":"Contribute/#tbd","title":"TBD","text":""},{"location":"Example/","title":"Examples","text":"<p>Several examples related to the above papers are presented here. Click the title link to see each example.</p>"},{"location":"Example/#1train-a-lstm-data-integration-model-to-make-streamflow-forecast","title":"1.Train a LSTM data integration model to make streamflow forecast","text":"<p>The dataset used is NCAR CAMELS dataset. Download CAMELS following this link.  Please download both forcing, observation data <code>CAMELS time series meteorology, observed flow, meta data (.zip)</code> and basin attributes <code>CAMELS Attributes (.zip)</code>.  Put two unzipped folders under the same directory, like <code>your/path/to/Camels/basin_timeseries_v1p2_metForcing_obsFlow</code>, and <code>your/path/to/Camels/camels_attributes_v2.0</code>. Set the directory path <code>your/path/to/Camels</code> as the variable <code>rootDatabase</code> inside the code later.</p> <p>Computational benchmark: training of CAMELS data (w/ or w/o data integration) with 671 basins, 10 years, 300 epochs, in ~1 hour with GPU.</p>"},{"location":"Installation/","title":"Installation","text":""},{"location":"Installation/#installation","title":"Installation","text":"<p>There are two different methods for hydroDL installation:</p>"},{"location":"Installation/#create-a-new-environment-then-activate-it","title":"Create a new environment, then activate it","text":"<pre><code>conda create -n mhpihydrodl python=3.10\nconda activate mhpihydrodl\n</code></pre>"},{"location":"Installation/#using-pypi-stable-package","title":"Using PyPI (stable package)","text":"<p>Install our hydroDL stable package from pip (Python version&gt;=3.0)</p> <pre><code>pip install hydroDL\n</code></pre>"},{"location":"Installation/#source-latest-version","title":"Source latest version","text":"<p>Install our latest hydroDL package from github</p> <pre><code>git clone https://github.com/mhpi/hydroDL.git\n</code></pre> <p>Note: If you want to run our examples directly, please download the example folder (It contains the code and data for these examples). </p> <p>There exists a small compatibility issue with our code when using the latest pyTorch version. Feel free to contact us if you find any issues or code bugs that you cannot resolve.</p>"},{"location":"Quick_start/","title":"Quick Start:","text":"<p>The detailed code for quick start can be found in tutorial_quick_start.py</p> <p>See below for a brief explanation of the major components you need to run a hydroDL model:</p> <pre><code># imports\nfrom hydroDL.model.crit import RmseLoss\nfrom hydroDL.model.rnn import CudnnLstmModel as LSTM\nfrom hydroDL.model.train import trainModel\nfrom hydroDL.model.test import testModel\n\n# load your training and testing data \n# x: forcing data (pixels, time, features)\n# c: attribute data (pixels, features)\n# y: observed values (pixels, time, 1)\nx_train, c_train, y_train, x_val, c_val, y_val = load_data(...)\n\n# define your model and loss function\nmodel = LSTM(nx=num_variables, ny=1)\nloss_fn = RmseLoss()\n\n# train your model\nmodel = trainModel(model,\n    x_train,\n    y_train,\n    c_train,\n    loss_fn,\n)\n\n# validate your model\npred = testModel(model,\n             x_val,\n             c_val,\n)\n</code></pre>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>We here provide comparisons to LSTM models on the CAMELS data (top of page) as well as comparisons to the current National Water Model at the national scale (bottom of this page), and more comparisons will be provided here.</p> <p>We recently updated our LSTM, and you can find the high-flow expert on hydroDL repo's tutorial (see Codes tab on this website). The first and forecast benchmark is over the CAMELS dataset. The results can vary slightly due to training/test periods. Below you will find results for 10-year training (exactly as reported in Kratzert et al., 2019) and 15-year training (shown in this Figure). Besides NSE and KGE, we also report absolute FHV and FLV (these metrics have + or - signs, and they make more sense after taking the absolute sign) and low-flow and high-flow RMSE. So far, the best LSTM is LSTM-hydroDL (high-flow expert) and the best differentiable model is \\(\\delta\\)HBV.adjoint (https://hess.copernicus.org/preprints/hess-2023-258/). As time goes on, we will also report benchmarks on the global dataset and other papers. We also know that spatial test (trained on some basins, tested on some other basins) or prediction in ungauged regions (PUR) tests (tested in a large region without training data) are more stringent tests and will likely change the comparisons. We previously found differentiable model to perform better in the PUR test (Feng et al., 2023 https://doi.org/10.5194/hess-27-2357-2023).</p>"},{"location":"benchmarks/#cdf-comparison","title":"CDF Comparison","text":"Camels NSE of popular streamflow models (single, without ensemble) wth 15-year training. This is a temporal test (trained on ). We compared 3 versions of differentiable HBV model (\"Unmodified\"-- without any structural update; $\\delta$HBV -- a sequential differentiable HBV published in Feng et al., 2022; and $\\delta$HBV.adjoint, slightly modified from Song et al., 2023. See refs below) with two versions of hydroDL implementation (a high-flow expert and a low-flow expert). We also trained the LSTM from Kratzert 2019 for comparison."},{"location":"benchmarks/#metric-tables","title":"Metric Tables","text":""},{"location":"benchmarks/#10-year-training-comparison","title":"10-year training comparison","text":"<p>Info</p> <p>All models were trained from 1999/10/01 to 2008/09/30 and tested from 1989/10/01 to 1999/09/30 on the subset of 531 CAMELS basins. </p> Model Median NSE Median KGE Median Absolute (Non-Absolute) FLV (%) Median Absolute (Non-Absolute) FHV (%) Median low flow RMSE (mm/day) Median peak flow RMSE (mm/day) LSTM-hydroDL-single (high-flow expert)\u202f 0.74 0.76 31.79 (-9.08) 16.20 (-13.42) 0.049 3.28 LSTM-hydroDL-Ensemble (high-flow expert)\u202f 0.765 0.77 28.84 (-3.88) 16.21 (-13.38) 0.046 3.27 LSTM-single ran w/ code in Kratzert et al. (2019) 0.74 0.75 32.02 (5.54) 18.02 (-15.80) 0.051 3.70 LSTM-single (Kratzert et al. 2019) As reported\u202f 0.731 - - (26.5) - (-14.8) - - LSTM-Ensemble (Kratzert et al. 2019) As reported\u202f 0.758 - - (55.1) - (-15.7) - -"},{"location":"benchmarks/#15-year-training-comparison","title":"15-year training comparison","text":"<p>Info</p> <p>All models were trained from 1980/10/01 to 1995/09/30 and tested from 1995/10/01 to 2010/09/30 on all 671 CAMELS basins.</p> Model Median NSE Median KGE Median Absolute (Non-Absolute) FLV (%) Median Absolute (Non-Absolute) FHV (%) Median low flow RMSE (mm/day) Median peak flow RMSE (mm/day) Baseflow index\u202fspatial correlation Median NSE of temporal ET simulation LSTM-hydroDL (low-flow expert) 0.73 0.76 19.52 (12.21) 15.01 (-4.12) 0.023 2.67 - - LSTM-hydroDL (high-flow expert)\u202f 0.74 0.78 37.33 (-20.72) 13.68 (-4.30) 0.048 2.49 - - LSTM ran w/ code in Kratzert et al. (2019) 0.73 0.77 40.59 (29.70) 13.46  (-4.19) 0.055 2.56 - - SAC-SMA (Traditional) 0.66 0.73 59.40 (46.96) 17.55 (-9.79) 0.081 3.19 - - Unmodified \\(\\delta\\)HBV 0.69 0.72 47.58 (16.84) 16.40 (-10.80) 0.066 2.74 0.76 0.43 \\(\\delta\\)HBV 0.73 0.73 56.53 (50.93) 15.29 (-8.89) 0.074 2.56 0.76 0.59 \\(\\delta\\)HBV.adj (expert 1) 0.72 0.75 43.29 (37.61) 13.25 (-4.33) 0.048 2.47 0.83 0.61 \\(\\delta\\)HBV.adj (expert 2) 0.75 0.76 40.56 (32.78) 14.09 (-7.97) 0.045 2.59 0.87 0.62"},{"location":"benchmarks/#citations","title":"Citations","text":"LSTM-hydroDL (low-flow expert)LSTM-hydroDL (high-flow expert)LSTM (Kratzert et al. 2019)SAC-SMA (Traditional)Unmodified \\(\\delta\\) HBV\\(\\delta\\) HBV\\(\\delta\\) HBV.adj <pre><code>Feng, Dapeng, Kuai Fang, and Chaopeng Shen. \"Enhancing streamflow forecast and extracting \ninsights using long\u2010short term memory networks with data integration at continental scales.\n\" Water Resources Research 56, no. 9 (2020): e2019WR026793. \n</code></pre> <pre><code>Github link:\n</code></pre> https://https://neuralhydrology.github.io/<pre><code>Kratzert, Frederik, Daniel Klotz, Guy Shalev, G\u00fcnter Klambauer, Sepp Hochreiter, and Grey \nNearing. \"Benchmarking a catchment-aware long short-term memory network (LSTM) for\nlarge-scale hydrological modeling.\" Hydrol. Earth Syst. Sci. Discuss 2019 (2019): 1-32.\n</code></pre> <pre><code>Newman, Andrew J., Martyn P. Clark, Kevin Sampson, Andrew Wood, Lauren E. Hay, Andy Bock, \nRoland J. Viger et al. \"Development of a large-sample watershed-scale hydrometeorological \ndata set for the contiguous USA: data set characteristics and assessment of regional \nvariability in hydrologic model performance.\" Hydrology and Earth System Sciences 19, no. 1 \n(2015): 209-223. \n</code></pre> <pre><code>Feng, D., Liu, J., Lawson, K. and Shen, C., 2022. Differentiable, learnable, regionalized \nprocess\u2010based models with multiphysical outputs can approach state\u2010of\u2010the\u2010art hydrologic \nprediction accuracy. Water Resources Research, 58(10), p.e2022WR032404. \n</code></pre> <pre><code>Feng, D., Liu, J., Lawson, K. and Shen, C., 2022. Differentiable, learnable, regionalized \nprocess\u2010based models with multiphysical outputs can approach state\u2010of\u2010the\u2010art hydrologic \nprediction accuracy. Water Resources Research, 58(10), p.e2022WR032404.\n</code></pre> <pre><code>Song, Yalan, Wouter Knoben, Martyn P. Clark, Dapeng Feng, Kathryn Lawson, and Chaopeng \nShen, When ancient numerical demons meet physics-informed machine learning: adjoint-based \ngradients for implicit differentiable modeling \nPreprint link: https://hess.copernicus.org/preprints/hess-2023-258/ \n</code></pre>"},{"location":"benchmarks/#comparison-with-national-water-models","title":"Comparison with National Water Models","text":"<p>Funded by CIROH projects, we have produced initial comparisons at the continental scale showing the superior performance of the differentiable models compared to both NOAA\u2019s first-generation WRF-Hydro.NWM Model, version 1.2 (Tijerina\u2010Kreuzer et al., 2021) and version 2.1 (Cosgrove et al., 2024). The differentiable routing model developed in our FY22 CIROH project is used for runoff routing using Muskingum-Cunge method. We are now producing seamless streamflow simulations at high spatial resolution for the whole CONUS and the results below are demonstrating one of the simulations. We are still improving the runoff, forcing, and routing aspects of the product. Several updates are incoming. Please stand by for a data release!</p> <p> </p> Condon diagrams comparing streamflow performance for $\\delta$HBV -- differentiable HBV and National Water Model, version 1.2. The $\\delta$HBV is trained from 10/1980 to 09/1995 and tested from 01/1981 to 12/2019. NWM Model, version 1.2 is uncalibrated and tested from 10/1984 to 09/1985 (reprinted from Tijerina\u2010Kreuzer et al., 2021)   <p> </p> Streamflow normalized Nash Sutcliffe Efficiency (NNSE) and correlation comparison between $\\delta$HBV -- differentiable HBV and NWM Model, version 2.1. The $\\delta$HBV is trained from 10/1980 to 09/1995 and tested from 01/1981 to 12/2019. NWM Model, version 2.1 is calibrated from 10/2008 to 09/2013 and tested from 10/2013 to 09/2016 (reprinted from Cosgrove et al., 2024)."},{"location":"blog/","title":"Blog","text":""},{"location":"codes/","title":"Code","text":"<p>See below for coding projects developed by the community that utilize HydroDL</p>"},{"location":"codes/#differentiable-modeling-framework","title":"Differentiable Modeling Framework","text":"<ul> <li>\ud835\udeffMG (Lonzarich et al. 2024)     ---  A second-generation generic, scalable differentiable modeling framework on PyTorch for integrating neural networks with physical models. Coupled with HydroDL2, \ud835\udeffMG enables hydrologic modeling like HydroDL while greatly expanding the range of applications and capabilities.</li> </ul>"},{"location":"codes/#differentiable-models","title":"Differentiable Models","text":"<ul> <li> <p>\\(\\delta\\) MC-Juniata-hydroDL2 (Bindas et al. 2024)</p> <p> A differentiable routing method that uses Muskingum-Cunge and an NN to infer parameterizations for Manning\u2019s roughness</p> </li> </ul> <ul> <li> <p>\\(\\delta\\) differentiable stream temperature (LSTM + SNTEMP) (Rahmani et al. 2023)</p> <p> CONUS scale pearson correlation between baseflow estimation of our model and GAGES-II estimates of baseflow.</p> </li> </ul> <ul> <li> <p>diffEcosys (Aboelyazeed_et_al_2023)</p> <p> A differentiable physics-informed ecosystem model </p> </li> </ul> <ul> <li> <p>\\(\\delta\\) HBV-hydroDL (Feng et al., 2022,2023)</p> <p> Differentiable hydrologic models using HBV model as a physical backbone.</p> </li> </ul> <ul> <li> <p>\\(\\delta\\) parameter learning (Tsai_et_al_2021)</p> <p> Differentiable Parameter Learning</p> </li> </ul> <ul> <li> <p>Updated, simplified \\(\\delta\\)HBV tutorial for CAMELS streamflow</p> <p>This notebook is a simplified \\(\\delta\\)HBV code tutorial for CAMELS streamflow. Thanks to Yalan Song and Dapeng Feng. Note that different pytorch versions could lead to slightly different performances. </p> </li> </ul>"},{"location":"codes/#lstm-models","title":"LSTM Models","text":"<ul> <li> <p>Starting point: Quick LSTM tutorial on soil moisture prediction</p> <p>This notebook is the starting point for new people to learn hydrologic time series prediction using deep neural networks. You can see how CudnnLSTMmodel and CpuLSTM models are trained and how data are formatted. Dataset is embedded in the hydroDL library so it is easy to run the example.</p> </li> </ul> <ul> <li> <p>Updated, simplified LSTM tutorial for CAMELS streamflow</p> <p>This notebook is the \"high-flow expert\" listed on the benchmark page. We greatly simplified the LSTM interface, making it easy to reuse this code on your data. This is slightly more involved than the soil moisture tutorial as we are dealing with a larger and more complex dataset. Thanks to Yalan Song, Kamlesh Sawadekar and Dapeng Feng. Note that different pytorch versions could lead to slightly different performances. </p> </li> </ul> <ul> <li> <p>Multiscale (Liu et al. 2022)</p> <p> A multiscale DL scheme learning simultaneously from satellite and in situ data to predict 9 km daily soil moisture (5 cm depth). </p> </li> </ul> <ul> <li> <p>LSTM for snow water equivalent (Song et al. 2023)</p> <p> Time series of the forward model and the models integrating snow water equivalent (SWE) and snow cover fraction (SCF) at different time lags. </p> </li> </ul> <ul> <li> <p>LSTM stream temperature model (Rahmani et al. 2021)</p> <p> CONUS scale aggregated metrics of stream temperature models for the testing time range considering observed, simulated, and no streamflow data among inputs,with a locally fitted auto-regressive model</p> </li> </ul>"},{"location":"codes/#transformer-models","title":"Transformer Models","text":"<ul> <li> <p>Transformer (Liu et al. 2024)</p> <p> First time Transformer achieved the same performance as LSTM on CAMELS dataset; LSTMs and Transformers are likely nearing the prediction limits of the dataset.</p> </li> </ul>"},{"location":"codes/Aboelyazeed_2023/","title":"diffEcosys","text":""},{"location":"codes/Aboelyazeed_2023/#short-summary","title":"Short Summary","text":"<p>Photosynthesis is critical for life and has been affected by the changing climate. Many parameters come into play while modeling, but traditional calibration approaches face many issues. Our framework trains coupled neural networks to provide parameters to a photosynthesis model. Using big data, we independently found parameter values that were correlated with those in the literature while giving higher correlation and reduced biases in photosynthesis rates.</p>"},{"location":"codes/Aboelyazeed_2023/#code-release","title":"Code Release","text":"<p>github version: https://github.com/hydroPKDN/diffEcosys/</p> <p>zenodo version: https://zenodo.org/records/8067204</p>"},{"location":"codes/Aboelyazeed_2023/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{Aboelyazeed2023,\n  doi = {10.5194/bg-20-2671-2023},\n  url = {https://doi.org/10.5194/bg-20-2671-2023},\n  year = {2023},\n  month = jul,\n  publisher = {Copernicus {GmbH}},\n  volume = {20},\n  number = {13},\n  pages = {2671--2692},\n  author = {Doaa Aboelyazeed and Chonggang Xu and Forrest M. Hoffman and Jiangtao Liu and Alex W. Jones and Chris Rackauckas and Kathryn Lawson and Chaopeng Shen},\n  title = {A differentiable,  physics-informed ecosystem modeling and  learning framework for large-scale inverse problems:  demonstration with photosynthesis simulations},\n  journal = {Biogeosciences}\n}\n</code></pre>"},{"location":"codes/Rahmani_et_al_2021/","title":"LSTM stream temperature model","text":""},{"location":"codes/Rahmani_et_al_2021/#code-release","title":"Code Release","text":"<p>See here for the USGS release</p>"},{"location":"codes/Rahmani_et_al_2021/#results","title":"Results","text":"<p>We developed a basin-centric LSTM model for daily stream temperature prediction and to evaluate the impact of streamflow information on the predictions.</p>"},{"location":"codes/Rahmani_et_al_2021/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{Rahmani2021lstm,\n  title={Exploring the Exceptional Performance of a Deep Learning Stream Temperature Model and the Value of Streamflow Data},\n  author={Rahmani, Farshid and Lawson, Kathryn and Ouyang, Wenyu and Appling, Alison and Oliver, Samantha and Shen, Chaopeng},\n  journal={Environmental Research Letters},\n  year={2021},\n  publisher={IOPScience}\n}\n</code></pre>"},{"location":"codes/Rahmani_et_al_2023/","title":"\\(\\delta\\) differentiable stream temperature (LSTM + SNTEMP)","text":""},{"location":"codes/Rahmani_et_al_2023/#code-data-models-and-results-release","title":"Code, data, models, and results Release","text":"<p>See here for the USGS release</p>"},{"location":"codes/Rahmani_et_al_2023/#summary","title":"Summary","text":"<p>We integrated a process-based stream water temperature model (SNTEMP) with LSTM module in a differentiable platform (PyTorch) to predict dily stream water temperature. The main difference between the differentiable model and many other previous physics-guided frameworks is that the process-based model is written in a differentiable platform like those used for NNs. This approach makes it possible for the framework to seamlessly integrate both machine learning and process-based model parts and be trained as a unified model (Shen et al., 2023). The training process is \u201cend-to-end,\u201d with any parameters within the model being learnable by gradient descent, just like when a pure LSTM model is trained. Thus, there is no need to calibrate each component independently. In other words, the process-based model becomes a part of the NN with the added advantage of process transparency.. All the hybrid models were trained on daily water temperature observations for 415 stations across the conterminous United States (CONUS). We evaluated the model with two metrics: the daily temperature simulation accuracy against observed data, and the correlation between the baseflow estimation of our model with a published alternative estimate based on baseflow recession analysis. </p>"},{"location":"codes/Rahmani_et_al_2023/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{Rahmani2021lstm,\n  title={Identifying Structural Priors in a Hybrid Differentiable Model for Stream Water Temperature Modeling},\n  author={Rahmani, Farshid and Appling, Alison and Feng, Dapeng and Lawson, Kathryn and Shen, Chaopeng},\n  journal={Water Resources Research},\n  year={2023},\n  publisher={AGU}\n}\n</code></pre>"},{"location":"codes/Song_SWE_2023/","title":"LSTM for snow water equivalent","text":""},{"location":"codes/Song_SWE_2023/#code-release","title":"Code Release","text":"<p>LSTM code for SWE is available at this link: https://github.com/mhpi/hydroDL/tree/release/example/snow_water_equivalent</p>"},{"location":"codes/Song_SWE_2023/#short-summary","title":"Short Summary","text":"<p>We tested an LSTM network with data integration (DI) for snow water equivalent (SWE) in the western US to integrate 30-day-lagged or 7-day-lagged observations of either SWE or satellite-observed snow cover fraction (SCF) to improve future predictions. SCF proved beneficial only for shallow-snow sites during snowmelt, while lagged SWE integration significantly improved prediction accuracy for both shallow-and deep-snow sites. The median Nash-Sutcliffe model efficiency coefficient (NSE) in temporal testing improved from 0.92 to 0.97 with 30-day-lagged SWE integration, and root-mean-square error (RMSE) and the difference between estimated and observed peak SWE values (dmax ) were reduced by 41% and 57%, respectively (https://journals.ametsoc.org/view/journals/hydr/25/1/JHM-D-22-0220.1.xml).</p>"},{"location":"codes/Song_SWE_2023/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{song2024lstm,\n  title={LSTM-based data integration to improve snow water equivalent prediction and diagnose error sources},\n  author={Song, Yalan and Tsai, Wen-Ping and Gluck, Jonah and Rhoades, Alan and Zarzycki, Colin and McCrary, Rachel and Lawson, Kathryn and Shen, Chaopeng},\n  journal={Journal of Hydrometeorology},\n  volume={25},\n  number={1},\n  pages={223--237},\n  year={2024},\n  publisher={American Meteorological Society}\n}\n</code></pre>"},{"location":"codes/Tsai_2021/","title":"\\(\\delta\\) parameter learning","text":""},{"location":"codes/Tsai_2021/#code-release","title":"Code Release","text":"<ul> <li>Codes are released at this zenodo link</li> </ul>"},{"location":"codes/Tsai_2021/#summary","title":"Summary","text":"<p>From calibration to parameter learning: Harnessing the scaling effects of big data in geoscientific modeling for routing flows on the river network.</p> <p>The behaviors and skills of models in many geosciences (e.g., hydrology and ecosystem sciences) strongly depend on spatially-varying parameters that need calibration.  A well-calibrated model can reasonably propagate information from observations to unobserved variables via model physics, but traditional calibration is highly inefficient  and results in non-unique solutions. Here we propose a novel differentiable parameter learning (dPL) framework that efficiently learns a global mapping between inputs  (and optionally responses) and parameters. Crucially, dPL exhibits beneficial scaling curves not previously demonstrated to geoscientists: as training data increases,  dPL achieves better performance, more physical coherence, and better generalizability (across space and uncalibrated variables), all with orders-of-magnitude lower  computational cost. We demonstrate examples that learned from soil moisture and streamflow, where dPL drastically outperformed existing evolutionary and regionalization methods, or required only ~12.5% of the training data to achieve similar performance. The generic scheme promotes the integration of deep learning and process-based models, without mandating reimplementation.</p>"},{"location":"codes/Tsai_2021/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{Tsai_2021,\n   title={From calibration to parameter learning: Harnessing the scaling effects of big data in geoscientific modeling},\n   volume={12},\n   ISSN={2041-1723},\n   url={http://dx.doi.org/10.1038/s41467-021-26107-z},\n   DOI={10.1038/s41467-021-26107-z},\n   number={1},\n   journal={Nature Communications},\n   publisher={Springer Science and Business Media LLC},\n   author={Tsai, Wen-Ping and Feng, Dapeng and Pan, Ming and Beck, Hylke and Lawson, Kathryn and Yang, Yuan and Liu, Jiangtao and Shen, Chaopeng},\n   year={2021},\n   month=oct }\n</code></pre>"},{"location":"codes/bindas_2023/","title":"\\(\\delta\\) MC-Juniata-hydroDL2","text":""},{"location":"codes/bindas_2023/#code-release","title":"Code Release","text":"<p>Below are Zenodo releases for:</p> <ul> <li>Code</li> <li>Dataset</li> </ul> <p>Check out the code on Github here</p>"},{"location":"codes/bindas_2023/#summary","title":"Summary","text":"<p>A novel differentiable modeling framework to perform routing and to learn a \u201cparameterization scheme\u201d (a systematic way of inferring parameters from more rudimentary information) for routing flows on the river network.</p> <p>Below are the main points from our paper:</p> <ul> <li>A novel differentiable routing model can learn effective river routing parameterization, recovering channel roughness in synthetic runs.</li> <li>With short periods of real training data, we can improve streamflow in large rivers compared to models not considering routing.</li> <li>For basins &gt;2,000 km2, our framework outperformed deep learning models that assume homogeneity, despite bias in the runoff forcings.</li> </ul>"},{"location":"codes/bindas_2023/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{https://doi.org/10.1029/2023WR035337,\nauthor = {Bindas, Tadd and Tsai, Wen-Ping and Liu, Jiangtao and Rahmani, Farshid and Feng, Dapeng and Bian, Yuchen and Lawson, Kathryn and Shen, Chaopeng},\ntitle = {Improving River Routing Using a Differentiable Muskingum-Cunge Model and Physics-Informed Machine Learning},\njournal = {Water Resources Research},\nvolume = {60},\nnumber = {1},\npages = {e2023WR035337},\nkeywords = {flood, routing, deep learning, physics-informed machine learning, Manning's roughness},\ndoi = {https://doi.org/10.1029/2023WR035337},\nurl = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2023WR035337},\neprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2023WR035337},\nnote = {e2023WR035337 2023WR035337},\nabstract = {Abstract Recently, rainfall-runoff simulations in small headwater basins have been improved by methodological advances such as deep neural networks (NNs) and hybrid physics-NN models\u2014particularly, a genre called differentiable modeling that intermingles NNs with physics to learn relationships between variables. However, hydrologic routing simulations, necessary for simulating floods in stem rivers downstream of large heterogeneous basins, had not yet benefited from these advances and it was unclear if the routing process could be improved via coupled NNs. We present a novel differentiable routing method (\u03b4MC-Juniata-hydroDL2) that mimics the classical Muskingum-Cunge routing model over a river network but embeds an NN to infer parameterizations for Manning's roughness (n) and channel geometries from raw reach-scale attributes like catchment areas and sinuosity. The NN was trained solely on downstream hydrographs. Synthetic experiments show that while the channel geometry parameter was unidentifiable, n can be identified with moderate precision. With real-world data, the trained differentiable routing model produced more accurate long-term routing results for both the training gage and untrained inner gages for larger subbasins (&gt;2,000\u00a0km2) than either a machine learning model assuming homogeneity, or simply using the sum of runoff from subbasins. The n parameterization trained on short periods gave high performance in other periods, despite significant errors in runoff inputs. The learned n pattern was consistent with literature expectations, demonstrating the framework's potential for knowledge discovery, but the absolute values can vary depending on training periods. The trained n parameterization can be coupled with traditional models to improve national-scale hydrologic flood simulations.},\nyear = {2024}\n}\n</code></pre>"},{"location":"codes/feng_2023/","title":"\\(\\delta\\) HBV-hydroDL","text":""},{"location":"codes/feng_2023/#code-release","title":"Code Release","text":"<p>The codes for differentiable hydrologic models are released at this zenodo link</p>"},{"location":"codes/feng_2023/#summary","title":"Summary","text":"<p>Differentiable hydrologic modeling provides a generic approach to unify machine learning and physical models. Here we first implemented the hydrologic model HBV in the PyTorch platform with Automatic Differentiation, so that we can freely embed neural network components to parameterize and evolve the structure of the original model. We flexibly evolved the original HBV structure under differentiable modeling by using multi-component computation and introducing dynamic parameters (Feng et al., 2022). These so-called differentiable hydrologic models can achieve state-of-the-art hydrologic simulation accuracy tested in public benchmark dataset while also keep physical process clarity. Different from the purely machine learning methods, differentiable models can provide accurate simulations for a full set of untrained hydrologic variables. We further compared the extrapolation ability of different models (prediction in ungauged regions) in both continental and global scales. Differentiable models stand out showing superior spatial generalization performance compared with traditional parameter regionalization and purely machine learning methods (Feng et al., 2023; Feng et al., 2023). These characteristics show differentiable models can be great candidates as the next-generation global hydrologic model.</p>"},{"location":"codes/feng_2023/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{feng2022WRR,\nauthor = {Feng, Dapeng and Liu, Jiangtao and Lawson, Kathryn and Shen, Chaopeng},\ntitle = {Differentiable, Learnable, Regionalized Process-Based Models With Multiphysical Outputs can Approach State-Of-The-Art Hydrologic Prediction Accuracy},\njournal = {Water Resources Research},\nvolume = {58},\nnumber = {10},\npages = {e2022WR032404},\nkeywords = {rainfall runoff, differentiable programming, machine learning, physical model, differentiable hydrology, LSTM},\ndoi = {https://doi.org/10.1029/2022WR032404},\nyear = {2022}\n}\n@Article{feng2023HESS,\nAUTHOR = {Feng, D. and Beck, H. and Lawson, K. and Shen, C.},\nTITLE = {The suitability of differentiable, physics-informed machine learning\nhydrologic models for ungauged regions and climate change impact assessment},\nJOURNAL = {Hydrology and Earth System Sciences},\nVOLUME = {27},\nYEAR = {2023},\nNUMBER = {12},\nPAGES = {2357--2373},\nURL = {https://hess.copernicus.org/articles/27/2357/2023/},\nDOI = {10.5194/hess-27-2357-2023}\n}\n@article{feng2023GMDD,\n  title={Deep Dive into Global Hydrologic Simulations: Harnessing the Power of Deep Learning and Physics-informed Differentiable Models ($\\delta$HBV-globe1. 0-hydroDL)},\n  author={Feng, Dapeng and Beck, Hylke and de Bruijn, Jens and Sahu, Reetik Kumar and Satoh, Yusuke and Wada, Yoshihide and Liu, Jiangtao and Pan, Ming and Lawson, Kathryn and Shen, Chaopeng},\n  journal={Geoscientific Model Development Discussions},\n  volume={2023},\n  pages={1--23},\n  year={2023},\n  publisher={G{\\\"o}ttingen, Germany}\n}\n</code></pre>"},{"location":"codes/frameworks/","title":"\ud835\udeffMG","text":""},{"location":"codes/frameworks/#the-generic-scalable-differentiable-modeling-framework","title":"The Generic, Scalable Differentiable Modeling Framework","text":"<p>Code Release </p> <p><code>\ud835\udeffMG</code> is a domain-agnostic, PyTorch-based framework for developing trainable differentiable models that merge neural networks with process-based equations. \ud835\udeffMG is not a partcularly model. Rather, it is a generic framework that support many models across various domains (some are from HydroDL2.0) in a uniform way, while integrating ecosystem tools. Although the packages contains some basic examples for learners' convenience, the deployment models are supposed to exit in separate repositories and couple to the \ud835\udeffMG framework. </p> <p>For example, the <code>hydroDL2</code> repository for hydrology models couples with the \ud835\udeffMG framework to enable MHPI-specific hydrologic modeling capabilities. The combination serves both as a benchmark capability for published results (including those that used hydroDL) and an exploratory platform for future hydrology research in MHPI. </p> <p>We include an optional GUI (source code) for constructing/editing \ud835\udeffMG YAML configuration files with a user-friendly interface:</p> <p>Closely synergizes with deep learning tools and the scale advantage of PyTorch. Maintained by the MHPI group advised by Dr. Chaopeng Shen.</p> <p></p>"},{"location":"codes/frameworks/#differentiable-models","title":"Differentiable Models","text":"<p>Characterized by the combination of process-based equations with neural networks (NNs), differentiable models train these components together, enabling parameter inputs for the equations to be effectively and efficiently learned at scale by the NNs. There are many possibilities for how such models are built.</p> <p>In \ud835\udeffMG, we define a differentiable model with the class DeltaModel that can couple one or more NNs with a process-based model (itself potentially a collection of models). This class holds <code>nn</code> and a <code>phy_model</code> objects, respectively, as attributes internally and describes how they interface with each other:</p> <ul> <li>nn: PyTorch neural networks that can learn and provide either parameters, missing process representations, corrections, or other forms of enhancements to physical models.</li> <li>phy_model: The physical model written in PyTorch (or potentially another interoperable differentiable platform) that takes learnable outputs from the <code>nn</code> model(s) and returns a prediction of some target variable(s). This can also be a wrapper holding several physical models.</li> </ul> <p>The DeltaModel object can be trained and forwarded just as any other PyTorch model (nn.Module).</p>"},{"location":"codes/liu_2022/","title":"Multiscale","text":""},{"location":"codes/liu_2022/#code-release","title":"Code Release","text":"<p>zenodo version</p>"},{"location":"codes/liu_2022/#short-summary","title":"Short Summary","text":"<p>Here we propose a novel multiscale DL scheme learning simultaneously from satellite and in situ data to predict 9 km daily soil moisture (5 cm depth). Based on spatial cross-validation over sites in the conterminous United States, the multiscale scheme obtained a median correlation of 0.901 and root-mean-square error of 0.034 m3/m3.</p>"},{"location":"codes/liu_2022/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{liu2022multiscale,\n  title={A multiscale deep learning model for soil moisture integrating satellite and in situ data},\n  author={Liu, Jiangtao and Rahmani, Farshid and Lawson, Kathryn and Shen, Chaopeng},\n  journal={Geophysical Research Letters},\n  volume={49},\n  number={7},\n  pages={e2021GL096847},\n  year={2022},\n  publisher={Wiley Online Library}\n}\n</code></pre>"},{"location":"codes/liu_2024/","title":"Transformer","text":""},{"location":"codes/liu_2024/#code-release","title":"Code Release","text":"<p>zenodo version</p>"},{"location":"codes/liu_2024/#short-summary","title":"Short Summary","text":"<p>This repository contains the code for our paper published in the Journal of Hydrology: Probing the limit of hydrologic predictability with the Transformer network</p>"},{"location":"codes/liu_2024/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{LIU2024131389,\n  title = {Probing the limit of hydrologic predictability with the Transformer network},\n  journal = {Journal of Hydrology},\n  volume = {637},\n  pages = {131389},\n  year = {2024},\n  issn = {0022-1694},\n  doi = {https://doi.org/10.1016/j.jhydrol.2024.131389},\n  url = {https://www.sciencedirect.com/science/article/pii/S0022169424007844},\n  author = {Jiangtao Liu and Yuchen Bian and Kathryn Lawson and Chaopeng Shen}\n}\n</code></pre>"},{"location":"datasets/CONUS/","title":"A continental Hydrologic dataset on seamless MERIT river network and ~180,000 MERIT unit basins","text":""},{"location":"datasets/CONUS/#preprint-paper","title":"Preprint paper","text":"<p>Song, Yalan, Tadd Bindas, Chaopeng Shen, Haoyu Ji, Wouter Johannes Maria Knoben, Leo Lonzarich, Martyn P. Clark et al. \"High-resolution national-scale water modeling is enhanced by multiscale differentiable physics-informed machine learning.\" Authorea Preprints (2024). https://doi.org/10.22541/essoar.172736277.74497104/v1</p>"},{"location":"datasets/CONUS/#results","title":"Results","text":""},{"location":"datasets/CONUS/#data-description","title":"Data description","text":"<p>\ud835\udeffHBV2_0_continental_data (zenodo and Colab code for data processing) is from High-resolution, multiscale, differentiable HBV hydrologic models, \ud835\udeffHBV2.0UH and \ud835\udeffHBV2.0dMC. \ud835\udeffHBV2.0UH is a high-resolution, multiscale model that uses unit hydrograph routing for. \ud835\udeffHBV2.0dMC is a high-resolution, multiscale model that uses external Muskingum-Cunge routing.</p> <p>The dHBV_streamflow_simulation_gages folder includes 40 years (1980\u20132020) of streamflow simulations at over 7,000 gage stations from GAGES-II, using both \ud835\udeffHBV2.0UH and \ud835\udeffHBV2.0dMC models. This data is useful for comparison with observations. The MERIT_flux_states folder includes 40 years (1980\u20132020) of spatially seamless simulations of hydrologic variables over 180 thousand MERIT unit basins on CONUS from \ud835\udeffHBV2.0UH, including baseflow, evapotranspiration (ET), soil moisture, snow water equivalent, and runoff. \ud835\udeffHBV2.0_MERIT_river_network_simulation folder includes 10 years (1990-1999) of streamflow simulation on seamless MERIT river network by \ud835\udeffHBV2.0dMC.</p>"},{"location":"datasets/CONUS/#code-release","title":"Code Release","text":"<p>Code release will come with the paper acceptance.</p>"},{"location":"dmg/code/","title":"Code Release","text":"<p>To start using \ud835\udeffMG, clone the framework <code>generic_deltaModel</code>:</p> <ul> <li>\ud835\udeffMG</li> </ul> <p>For tutorials and MHPI benchmarks, additionally clone the hydrologic model package <code>hydroDL2</code> and download our CAMELS data extraction:</p> <ul> <li>HydroDL 2.0</li> <li>CAMELS Date </li> </ul> <p>We include an optional GUI for constructing/editing \ud835\udeffMG YAML configuration files with a user-friendly interface:</p> <ul> <li>GUI Config builder (Zip)</li> <li>(Source Code)</li> </ul>"},{"location":"dmg/detail/","title":"\ud835\udeffMG","text":"<p>A domain-agnostic, PyTorch-based framework for developing trainable differentiable models that merge neural networks with process-based equations. \"Differentiable\" means that gradient calculations can be achieved efficiently at large scale throughout the model, so process-based equations can be trained together with NNs on big data, on GPU.  Following as a generalization of <code>HydroDL</code>, \ud835\udeffMG (<code>generic_deltaModel</code>) aims to expand differentiable modeling and learning capabilities to a wide variety of domains where prior equations can bring in benefits.</p> <p></p>"},{"location":"dmg/detail/#ecosystem-integration","title":"Ecosystem Integration","text":"<p>For differentiable hydrology models used in MHPI research, \ud835\udeffMG seamlessly integrates with:</p> <ul> <li>HydroDL2.0 (<code>hydroDL2</code>): Home to MHPI's suite of physics-based hydrology models, and differentiable model augmentations (think variational data       assimilation, model coupling, and additional physics-based hydrology tools).</li> <li>HydroData (<code>hydro_data_dev</code>): Data extraction, processing, and management tools optimized for geospatial datasets.</li> <li>Config GUI (<code>GUI-Config-builder</code>)(Source): An intuitive, user-friendly tool designed to simplify the creation and editing of configuration files for model setup and development.</li> <li>Concurrent development activities: We are working on these efforts connected to \ud835\udeffMG: (i) numerical PDE solvers on torch; (ii) adjoint sensitivity; (iii) extremely efficient and highly accurate surrogate models; (iv) downscaled and bias corrected climate data; (v) mysteriously powerful neural networks, and more...</li> </ul> <p></p>"},{"location":"dmg/detail/#key-features","title":"Key Features","text":"<ul> <li> <p>Hybrid Modeling: Combines neural networks with physical process equations for enhanced interpretability and generalizability. Skip manually tuning model parameters by using neural networks to feed robust and interpretable parameter predictions directly.</p> </li> <li> <p>PyTorch Integration: Easily scales with PyTorch, enabling efficient training and compatibility with modern deep learning tools, trained foundation models, differentiable numerical solvers.</p> </li> <li> <p>Domain-agnostic and Flexible: Extends differentiable modeling to any field where physics-guided learning can add value, with modularity to meet the diversity of needs along the way.</p> </li> <li> <p>Benchmarking: All in one place. \ud835\udeffMG + hydroDL2 will enable rapid deployment and replication of key published MHPI results.</p> </li> <li> <p>NextGen-ready: \ud835\udeffMG is designed to be CSDMS BMI-compliant, and our differentiable hydrology models in hydroDL2 come with a prebuilt BMI allowing seamless compatibility with NOAA-OWP's NextGen National Water Modelling Framework. Incidentally, this capability also lends to \ud835\udeffMG being easily wrappable for other applications.</p> </li> </ul> <p></p>"},{"location":"dmg/detail/#use-cases","title":"Use Cases","text":"<p>This package powers the global- and  (<code>national-scale water model</code>) that provide high-quality seamless hydrologic simulations over US and the world. It also hosts (<code>global-scale photosynthesis</code>) learning and simulations</p> <p></p>"},{"location":"dmg/detail/#the-overall-idea","title":"The Overall Idea","text":"<p>Characterized by the combination of process-based equations with neural networks (NNs), differentiable models train these components together, enabling parameter inputs for the equations to be effectively and efficiently learned at scale by the NNs. There are many possibilities for how such models are built.</p> <p>In \ud835\udeffMG, we define a differentiable model with the class DeltaModel that can couple one or more NNs with a process-based model (itself potentially a collection of models). This class holds <code>nn</code> and a <code>phy_model</code> objects, respectively, as attributes internally and describes how they interface with each other. The DeltaModel object can be trained and forwarded just as any other PyTorch model (nn.Module).</p> <p>We also define DataLoader and DataSampler classes to handle datasets, a Trainer class for running train/test experiments, and a ModelHandler class for multimodel handling, multi-GPU training, data assimilation and streaming in a uniform and modular way. All model, training, and simulation settings are be collected in a configuration file that can be adapted to custom applications.  According to this schema, we define these core classes, from bottom up:</p> <ul> <li>nn: PyTorch neural networks that can learn and provide either parameters, missing process representations, corrections, or other forms of enhancements to physical models.</li> <li>phy_model: The physical model written in PyTorch (or potentially another interoperable differentiable platform) that takes learnable outputs from the <code>nn</code> model(s) and returns a prediction of some target variable(s). This can also be a wrapper holding several physical models.</li> <li>DeltaModel: Holds (one or multiple) <code>nn</code> objects and a <code>phy_model</code> object, and describes how they are coupled; connection to ODE packages.</li> <li>ModelHandler: Manages multimodeling, multi-GPU compute, and data assimilation or streaming. Can contain its own optimizers. Acts as an interface to CSDMS BMI or other interfaces.</li> <li>DataSampler: Samples data according to data format and training/testing requirements.</li> <li>Trainer: Manages model training and testing, and connects data to models.</li> <li>DataLoader: Preprocesses data to be used in training, testing, and simulation.</li> </ul> <p></p>"},{"location":"dmg/detail/#mg-repository-structure","title":"\ud835\udeffMG Repository Structure:","text":"<pre><code>.\n\u251c\u2500\u2500 deltaModel/\n\u2502   \u251c\u2500\u2500 __main__.py                 # Run the framework; model experiments\n\u2502   \u251c\u2500\u2500 conf/                       # Configuration repository\n\u2502   \u2502   \u251c\u2500\u2500 config.py\n\u2502   \u2502   \u251c\u2500\u2500 config.yaml             # Main configuration file\n\u2502   \u2502   \u251c\u2500\u2500 hydra/                  \n\u2502   \u2502   \u2514\u2500\u2500 observations/           # Data configuration files\n\u2502   \u251c\u2500\u2500 core/                       \n\u2502   \u2502   \u251c\u2500\u2500 calc/                   # Calculation utilities\n\u2502   \u2502   \u251c\u2500\u2500 data/                   # Data Loaders and Samplers\n\u2502   \u2502   \u2514\u2500\u2500 utils/                  # Helper functions\n\u2502   \u251c\u2500\u2500 models/                     \n\u2502   \u2502   \u251c\u2500\u2500 differentiable_model.py # Differentiable model (dPL modality)\n\u2502   \u2502   \u251c\u2500\u2500 model_handler.py        # High-level model manager\n\u2502   \u2502   \u251c\u2500\u2500 loss_functions/         # Custom loss functions\n\u2502   \u2502   \u2514\u2500\u2500 neural_networks/        # Neural network architectures\n\u2502   \u2514\u2500\u2500 trainers/                   # Training routines\n\u251c\u2500\u2500 docs/                           \n\u251c\u2500\u2500 envs/                           # Environment configuration files\n\u2514\u2500\u2500 example/                        # Example and tutorial scripts\n</code></pre>"},{"location":"dmg/detail/#getting-started-with-mg-hydrodl-20","title":"Getting Started with \ud835\udeffMG + HydroDL 2.0","text":""},{"location":"dmg/detail/#system-requirements","title":"System Requirements","text":"<p>\ud835\udeffMG uses PyTorch models requiring CUDA exclusively supported by NVIDIA GPUs. This requires using</p> <ul> <li> <p>Windows or Linux</p> </li> <li> <p>NVIDIA GPU(s) (with CUDA version &gt;12.0 recommended)</p> </li> </ul> <p></p>"},{"location":"dmg/detail/#setup","title":"Setup","text":""},{"location":"dmg/detail/#1-clone-and-download-example-data","title":"1. Clone and Download Example Data","text":"<ul> <li> <p>Open a terminal on your system, navigate to the directory where \ud835\udeffMG and HydroDL2 will be stored, and clone (<code>master</code> branch):</p> <pre><code>git clone https://github.com/mhpi/generic_deltaModel.git\ngit clone https://github.com/mhpi/hydroDL2.git\n</code></pre> </li> <li> <p>Download the CAMELS data zip from the link above and extract, optionally to a <code>data/</code> folder in your working directory, which should now look something like</p> </li> </ul> <pre><code>    .\n    \u251c\u2500\u2500 data/\n    \u2502   \u251c\u2500\u2500 training_file           # Pickle file with training data\n    \u2502   \u251c\u2500\u2500 validation_file         # Pickle file with validation/testing data\n    \u2502   \u251c\u2500\u2500 gage_ids.npy            # Numpy array with all 671 CAMELS gage ids\n    \u2502   \u2514\u2500\u2500 531_subset.txt          # Text file of gage ids in 531-gage subset\n    \u251c\u2500\u2500 generic_deltaModel/\n    \u2514\u2500\u2500 hydroDL2/ \n</code></pre>"},{"location":"dmg/detail/#2-install-the-env","title":"2. Install the ENV","text":"<ul> <li>A minimal yaml list of essential packages is included in <code>generic_deltaModel</code>: <code>generic_deltaModel/envs/deltamodel_env.yaml</code>.</li> <li> <p>To install, run the following (optionally, include <code>--prefix</code> flag to specify the env download location):</p> <p><pre><code>conda env create --file /generic_deltaModel/envs/deltamodel_env.yaml\n</code></pre>  or</p> <pre><code>conda env create --prefix path/to/env --file /generic_deltaModel/envs/deltamodel_env.yaml\n</code></pre> </li> <li> <p>Activate with <code>conda activate deltamodel</code> and open a Python instance to check that CUDA is available with PyTorch:</p> <pre><code>import torch\nprint(torch.cuda.is_available())\n</code></pre> </li> <li> <p>If CUDA is not available, often PyTorch is installed incorrectly. Uninstall PyTorch from the env and reinstall according to your system specifications here. E.g.,</p> <pre><code>conda uninstall pytorch\nconda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n</code></pre> </li> </ul>"},{"location":"dmg/detail/#3-install-hydrodl2","title":"3. Install HydroDL2","text":"<ul> <li> <p>For <code>hydroDL2</code> to be accessible within <code>generic_deltaModel</code>, install with pip (optionally, include <code>-e</code> flag to install with developer mode):</p> <pre><code>cd hydroDL2\npip install .\n</code></pre> <p>or</p> <pre><code>cd hydroDL2\npip install -e .\n</code></pre> </li> </ul>"},{"location":"dmg/detail/#4-build-models","title":"4. Build Models","text":"<ul> <li>That's it. You should now be able to run the tutorials, train/test MHPI benchmarks, and build your own differentiable models.</li> </ul>"},{"location":"dmg/detail/#using-the-config-gui","title":"Using the Config GUI","text":""},{"location":"dmg/detail/#setup_1","title":"Setup","text":"<p>To use the HydroDL Config Builder from the GitHub source code, you have two options:</p> <ul> <li>Run Directly: Execute the tool using main.py.</li> <li>Build a Windows Executable: Use build.py to generate a standalone Windows executable.</li> </ul> <p>Alternatively, you can skip the build process by downloading the precompiled executable here. Once downloaded, simply unzip and run the executable  <code>HydroDL Config Builder.exe</code> on Windows to start the builder and begin creating/editing your configuration files. </p> <p>Two files can potentiallly be created by this process. One contains model and experiment settings, while the other is a data config that specifies dataset specific information like data save paths.</p>"},{"location":"dmg/detail/#where-do-config-files-go","title":"Where do Config Files go?","text":"<p>Once you have created and saved your YAML config files, they can go one of two places depending on your intentions. </p> <ul> <li>Tutorials: <code>example/conf</code>, with data config in <code>example/conf/observations</code>.</li> <li>Development with \ud835\udeffMG: <code>deltaModel/conf</code> with data config in <code>deltaModel/conf/observations</code></li> </ul> <p>Note. Before running \ud835\udeffMG, ensure that 'observations' in the main config matches the name of the data config you want to use.</p> <p></p>"},{"location":"dmg/detail/#quick-start-building-a-differentiable-hbv-hbv-model","title":"Quick Start: Building a Differentiable HBV (\ud835\udeffHBV) Model","text":"<p>Here\u2019s an example of how you can build a differentiable model, coupling a physical model with a neural network to intelligently learn parameters. In this instance, we use an LSTM to learn parameters for the HBV hydrology model. <pre><code>from example import load_config \nfrom hydroDL2.models.hbv.hbv import HBV as hbv\nfrom deltaModel.models.neural_networks import init_nn_model\nfrom deltaModel.models.differentiable_model import DeltaModel\nfrom deltaModel.core.data.data_loaders.hydro_loader import HydroDataLoader\nfrom deltaModel.core.data.data_samplers.hydro_sampler import take_sample\n\n\nCONFIG_PATH = '../example/conf/config_dhbv1_1p.yaml'\n\n\n# 1. Load configuration dictionary of model parameters and options.\nconfig = load_config(CONFIG_PATH)\n\n# 2. Setup a dataset dictionary of NN and physics model inputs.\ndataset = HydroDataLoader(config, test_split=True).eval_dataset\ndataset_sample = take_sample(config, dataset, days=730, basins=100)\n\n# 3. Initialize physical model and NN.\nphy_model = hbv(config['dpl_model']['phy_model'])\nnn = init_nn_model(phy_model, config['dpl_model'])\n\n# 4. Create the differentiable model dHBV: a torch.nn.Module that describes how \n# the NN is linked to the physical model HBV.\ndpl_model = DeltaModel(phy_model=phy_model, nn_model=nn)\n\n\n## From here, forward or train dpl_model just as any torch.nn.Module model.\n\n# 5. For example, to forward:\noutput = dpl_model.forward(dataset_sample)\n</code></pre></p> <p>In the above, we illustrate a critical behavior of the differentiable model object <code>DeltaModel</code>, which is the the composition of the physical model, <code>phy_model=hbv</code>, with a neural network, <code>nn</code>. </p> <p>When we forward DeltaModel, we feed scaled inputs (stored within the dataset dictionary) to the NN and forward, which returns a set of predicted parameters. These parameters then pass with the dataset dictionary to forward the phy_model and output final model predictions. Internally, these steps are represented within DeltaModel forward method as</p> <pre><code># Parameterization\nparameters = self.nn_model(dataset_sample['xc_nn_norm'])        \n\n# Physics model forward\npredictions = self.phy_model(\n    dataset_sample,\n    parameters,\n)\n</code></pre> <p>See examples/ in the <code>generic_deltaModel</code> repository for this and other tutorials.</p> <p>Note, the Config GUI can be used to create/edit additional config files for use with these examples. (Usage instructions)</p> <p></p> <p>Explore the roadmap for planned features and improvements. Differentiable numerical packages like torchode and torchdiffeq will be coming additions in the near future.</p>"},{"location":"docs/","title":"Docs","text":"<p>HydroDL's differentiable modeling interface is set up to provide a seemless template for using physics models and neural networks together. </p> <p>Our framework provides a standard for differentiable physics models to follow such that anyone can plug their models together. </p>"},{"location":"docs/#sections","title":"Sections","text":"<ul> <li>Datasets</li> <li>Neural Networks</li> <li>Physics Models</li> <li>Experiments</li> <li>Configs</li> </ul>"},{"location":"docs/#plugins","title":"Plugins","text":"<p>Plugins are a way to build off of open-source Deep Learning papers and repositories. </p>"},{"location":"docs/datasets/","title":"Datasets","text":"<p>The Datasets used in hydroDL are individual <code>@dataclass</code> classes used to create a Pytorch <code>torch.utils.data.Dataloader</code>. The classe</p> <code>Data</code> <p>Inputs to the neural networks</p> <code>Observations</code> <p>Targets used when training</p>"},{"location":"docs/datasets/#data","title":"Data","text":"<p>Data classes are implementations of the following <code>ABC</code> class:</p> __init__.Data.py<pre><code>from abc import ABC, abstractmethod\n\nfrom omegaconf import DictConfig\nimport torch\n\nclass Data(ABC):\n     @abstractmethod\n    def __init__(self, cfg: DictConfig, dates: Dates, normalize: Normalize):\n        \"\"\"A function to define what inputs are required by a Data object\"\"\"\n        pass\n\n    @abstractmethod\n    def _read_attributes(self) -&gt; None:\n        \"\"\"\n        Abstract method for reading attributes related to the data.\n\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _read_forcings(self) -&gt; None:\n        \"\"\"\n        Abstract method for reading attributes related to the data.\n        :return: None\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _read_data(self) -&gt; None:\n        \"\"\"The method to read all data\"\"\"\n        pass\n\n    @abstractmethod\n    def get_data(self) -&gt; Hydrofabric:\n        \"\"\"\n        Abstract method for retrieving data in the form of a hydrofabric\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"docs/experiments/","title":"Experiments","text":"<p>HydroDL experiments are designed to seamlessly be both reusableand structured. All experiments are child classes of the base <code>Experiment</code> class:</p> __init__.Experiment.py<pre><code>from abc import ABC, abstractmethod\nfrom typing import Dict, Type\n\nimport torch\nimport torch.nn\n\nclass Experiment(ABC):\n    @abstractmethod\n    def run(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        neural_network: nn.Module,\n        physics_models: Dict[str, Type[nn.Module]],\n    ) -&gt; None:\n        \"\"\"a method that runs your experiment\"\"\"\n        pass\n</code></pre> <p>The arguments passed into the parameters of the run function are all either class references (<code>physics_models</code>) or full instantiated classes (<code>data_loader</code>, or <code>neural_network</code>)</p>"},{"location":"docs/neural_networks/","title":"Neural Networks","text":"<p>Neural Networks are configured similar to how they are instantiated in other PyTorch packages. </p> <pre><code>from functools import partial\n\nfrom omegaconf import DictConfig\nimport torch\nimport torch.nn as nn\n\nfrom hydroRoute.neural_networks import Initialization\n\nclass NN(nn.Module):\n    def __init__(self, cfg: DictConfig):\n        super(MLP, self).__init__()\n        self.cfg = cfg\n        self.Initialization = Initialization(self.cfg)\n\n    def initialize_weights(self) -&gt; None:\n        \"\"\"\n        The partial function used to \n        \"\"\"\n        func = self.Initialization.get()\n        init_func = partial(self._initialize_weights, func=func)\n        self.apply(init_func)\n\n    @staticmethod\n    def _initialize_weights(m, func) -&gt; None:\n        \"\"\"\n        An internal class used to intialize weights based\n        on a provided initialization function\n        \"\"\"\n        if isinstance(m, nn.Linear):\n            func(m.weight)\n\n    def forward(self, inputs: torch.Tensor) -&gt; None:\n        pass\n</code></pre>"},{"location":"docs/physics_models/","title":"Physics Models","text":"<p>HydroDL's implemented physics models are all child classes of the Pytorch <code>nn.Module</code> class. By creating your physics model as an nn.Module, you can tap into PyTorch's neural network functionality and get a lot of bonuses. </p>"},{"location":"docs/physics_models/#basics","title":"Basics","text":"<p>Our physics models are structured as follows:</p> <pre><code>from typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nclass PhysicsModel(nn.Module):\n    def __init__(self, cfg: DictConfig) -&gt; None:\n        super(PhysicsModel, self).__init__()\n        self.cfg = cfg\n\n    def forward(self, inputs: Tuple[..., torch.Tensor]) -&gt; torch.Tensor:\n</code></pre> <p>Where all that is required by a physics model is it's specified configuration file. Since there are different requirements for each physics model, it is necessary for you to read into the specific configurations required by each module. </p>"},{"location":"docs/plugins/","title":"Plugins","text":""},{"location":"docs/plugins/hydrodl/","title":"HydroDL","text":""}]}